{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_code = \"\"\"\n",
    "data {\n",
    "    int n;\n",
    "    int y[n];\n",
    "    vector[n] logEi;\n",
    "    vector<lower=0>[n] income;\n",
    "    vector<lower=0>[n] homevalue;\n",
    "    vector<lower=0>[n] poverty;\n",
    "    vector<lower=0>[n] unemployed;\n",
    "    vector<lower=0>[n] education;\n",
    "    matrix<lower=0>[n,n] wmat;\n",
    "}\n",
    "parameters {\n",
    "    vector[n] theta;\n",
    "    vector[n] u;\n",
    "    real beta1;\n",
    "    real beta3;\n",
    "    real beta4;\n",
    "    real beta5;\n",
    "    real beta6;\n",
    "    real<lower=0> sigma2_u;\n",
    "    real<lower=0> sigma2_v;\n",
    "}\n",
    "model {\n",
    "    sigma2_u ~ inv_gamma(0.0005, 0.5);\n",
    "    sigma2_v ~ inv_gamma(0.0005, 0.5);\n",
    "    beta1 ~ uniform(-10, 10);\n",
    "    beta3 ~ uniform(-10, 10);\n",
    "    beta4 ~ uniform(-10, 10);\n",
    "    beta5 ~ uniform(-10, 10);\n",
    "    beta6 ~ uniform(-10, 10);\n",
    "\n",
    "    target += -0.5 * n * log(sigma2_u);\n",
    "    for (i in 1:n) {\n",
    "        for (j in 1:n) {\n",
    "            target += -0.5 * (u[i] - u[j])^2 * wmat[i, j] / sigma2_u;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for (i in 1:n) {\n",
    "        theta[i] ~ normal(logEi[i] + beta1 * income[i] + beta3 * homevalue[i] + beta4 * poverty[i] + beta5 * unemployed[i] + beta6 * education[i] + u[i], sqrt(sigma2_v));\n",
    "        y[i] ~ poisson(exp(theta[i]));\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "# Shapefiles\n",
    "shapefile_path = 'data/neighborhoods'\n",
    "sf_neighborhoods = gpd.read_file(shapefile_path)\n",
    "\n",
    "# SF case and population data\n",
    "sf_cases = pd.read_csv('data/master_merged.csv', index_col=False)\n",
    "sf_cases.rename(columns={'Neighborhood': 'nhood'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_cases = sf_cases.drop(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating distance matrix\n",
    "sf_neighborhoods_projected = sf_neighborhoods.to_crs(epsg=32610)\n",
    "sf_neighborhoods_projected['centroid'] = sf_neighborhoods_projected.geometry.centroid\n",
    "\n",
    "neighborhood_x = sf_neighborhoods_projected['centroid'].geometry.x\n",
    "neighborhood_y = sf_neighborhoods_projected['centroid'].geometry.y\n",
    "\n",
    "neighborhood_xy = np.column_stack((neighborhood_x, neighborhood_y))\n",
    "\n",
    "distance_matrix = distance.cdist(neighborhood_xy, neighborhood_xy, 'euclidean')\n",
    "distance_matrix /=  1000.0\n",
    "\n",
    "# Compute weight matrix based off of if they are under 20% quantile\n",
    "quantile_20 = np.quantile(distance_matrix, 0.2)\n",
    "weight_matix = (distance_matrix < quantile_20).astype(int)\n",
    "weight_matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_neighborhoods = pd.merge(sf_neighborhoods, sf_cases, on = 'nhood', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_neighborhoods['Homeless Cases'] = sf_neighborhoods['Homeless Cases'].replace(0,2)\n",
    "sf_neighborhoods['Homeless Cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'n': 41,\n",
    "    'y': np.array(sf_neighborhoods['Homeless Cases']).astype(int).tolist(),\n",
    "    'logEi': sf_neighborhoods['Log Expected Cases'].tolist(),\n",
    "    'income': sf_neighborhoods['Median Household Income'].tolist(),\n",
    "    #'foreign': sf_neighborhoods['Foreign Born'].tolist(),\n",
    "    'homevalue': sf_neighborhoods['Median Home Value'].tolist(),\n",
    "    'poverty': sf_neighborhoods['Percent in Poverty'].tolist(),\n",
    "    'unemployed': sf_neighborhoods['A_Unemployment Rate'].tolist(),\n",
    "    'education': sf_neighborhoods['Bachelor\\'s degree or higher'].tolist(),\n",
    "    'wmat': weight_matix.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = pystan.StanModel(model_code=SF_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.sampling(data=data, chains=1, iter=15000, warmup=7500, control={'max_treedepth': 14, 'adapt_delta': 0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fit.extract(permuted=True)  \n",
    "new_samples = {} \n",
    "\n",
    "for key, value in samples.items():\n",
    "    if len(value.shape) > 1:  \n",
    "        for i in range(value.shape[1]):\n",
    "            new_samples[f\"{key}_{i}\"] = value[:, i]\n",
    "    else:\n",
    "        new_samples[key] = value \n",
    "\n",
    "samples.update(new_samples)  \n",
    "\n",
    "keys_to_remove = [key for key, value in samples.items() if isinstance(value, np.ndarray) and len(value.shape) > 1]\n",
    "for key in keys_to_remove:\n",
    "    del samples[key]\n",
    "\n",
    "samples_df = pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "plt.plot(samples_df['sigma2_u'])\n",
    "plt.title('Trace of $\\Theta_{0}$')  \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('$\\Theta_{i}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.to_csv('model_results/stan_model_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = az.from_pystan(fit)\n",
    "az.plot_trace(idata, var_names=['theta'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['beta1'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['beta3'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['beta4'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['beta5'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['beta6'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['sigma2_u'])\n",
    "plt.show()\n",
    "\n",
    "az.plot_trace(idata, var_names=['sigma2_v'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the betas for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.read_csv('model_results/stan_model_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = ['beta1', 'beta3', 'beta4', 'beta5', 'beta6']\n",
    "\n",
    "results = {}\n",
    "for beta in betas:\n",
    "    mean = samples_df[beta].mean()\n",
    "    ci_lower = np.percentile(samples_df[beta], 2.5)\n",
    "    ci_upper = np.percentile(samples_df[beta], 97.5)\n",
    "    results[beta] = {'mean': mean, '95% CI': (ci_lower, ci_upper)}\n",
    "\n",
    "for beta, vals in results.items():\n",
    "    print(f\"{beta}: Mean = {vals['mean']}, 95% Credible Interval = {vals['95% CI']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
